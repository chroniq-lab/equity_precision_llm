{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn as sklearn\n",
    "import os as os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re as re\n",
    "\n",
    "if os.getlogin()==\"JVARGH7\":\n",
    "   path_equity_precision_llm_folder = \"C:/Cloud/OneDrive - Emory University/Papers/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "   path_equity_precision_llm_repo =  'C:/code/external/equity_precision_llm'\n",
    "\n",
    "elif os.getlogin()=='aamnasoniwala':\n",
    "   path_equity_precision_llm_folder = \"/Users/aamnasoniwala/Library/CloudStorage/OneDrive-Emory/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "   path_equity_precision_llm_repo = '/Users/aamnasoniwala/code/equity_precision_llm'\n",
    "\n",
    "# path_equity_precision_llm_folder = \"/Users/aamnasoniwala/Library/CloudStorage/OneDrive-Emory/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "# path_equity_precision_llm_repo = '/Users/aamnasoniwala/code/equity_precision_llm'\n",
    "\n",
    "excel_path = path_equity_precision_llm_folder + \"/llm training/Test Data Splits/Test Part \"\n",
    "# path_equity_precision_llm_repo = os.path.abspath(\"\").replace(\"preprocessing\", \"\")\n",
    "\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/crosstab_summary.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path_test = path_equity_precision_llm_folder + \"/llm training/epldat03_Test Data.xlsx\"\n",
    "# path_equity_precision_llm_repo = os.path.abspath(\"\").replace(\"preprocessing\", \"\")\n",
    "\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/clean_input.py\")\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/crosstab_summary.py\")\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/standardize_population.py\")\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/adjusted_source_population_match.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvargh7\\AppData\\Local\\Temp\\ipykernel_3732\\4106794397.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  input_test['Correct Source Population'].fillna('NA', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>MeSH</th>\n",
       "      <th>Precision Medicine</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Primary Study</th>\n",
       "      <th>Correct Source Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>318154</td>\n",
       "      <td>Mannosidosis: assignment of the lysosomal alph...</td>\n",
       "      <td>Human alpha-mannosidase activity (alpha-D-mann...</td>\n",
       "      <td>Animals, Carbohydrate Metabolism, Inborn Error...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>EA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>474468</td>\n",
       "      <td>Adipose tissue cellularity in obese nondiabeti...</td>\n",
       "      <td>In a Pacific island (Polynesian) population ex...</td>\n",
       "      <td>Adipose Tissue, Adult, Blood Glucose, Body Hei...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SEAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>739910</td>\n",
       "      <td>Lifestyle, health and disease: a comparison be...</td>\n",
       "      <td>The proposition that lifestyle is a major dete...</td>\n",
       "      <td>Adolescent, Adult, Australia, Cardiovascular D...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>SEAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080357</td>\n",
       "      <td>The effect of menopause on serum cholesterol i...</td>\n",
       "      <td>Serum cholesterol levels in Pima Indian women ...</td>\n",
       "      <td>Adult, Arizona, Cholesterol, Cholesterol, Diet...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1132094</td>\n",
       "      <td>Follow-up of mass screening for coronary risk ...</td>\n",
       "      <td>The prevalence of coronary risk factors was as...</td>\n",
       "      <td>Adult, Age Factors, Aged, Arteriosclerosis, Bl...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PMID                                              Title  \\\n",
       "0   318154  Mannosidosis: assignment of the lysosomal alph...   \n",
       "1   474468  Adipose tissue cellularity in obese nondiabeti...   \n",
       "2   739910  Lifestyle, health and disease: a comparison be...   \n",
       "3  1080357  The effect of menopause on serum cholesterol i...   \n",
       "4  1132094  Follow-up of mass screening for coronary risk ...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Human alpha-mannosidase activity (alpha-D-mann...   \n",
       "1  In a Pacific island (Polynesian) population ex...   \n",
       "2  The proposition that lifestyle is a major dete...   \n",
       "3  Serum cholesterol levels in Pima Indian women ...   \n",
       "4  The prevalence of coronary risk factors was as...   \n",
       "\n",
       "                                                MeSH Precision Medicine  \\\n",
       "0  Animals, Carbohydrate Metabolism, Inborn Error...                Yes   \n",
       "1  Adipose Tissue, Adult, Blood Glucose, Body Hei...                 No   \n",
       "2  Adolescent, Adult, Australia, Cardiovascular D...                 No   \n",
       "3  Adult, Arizona, Cholesterol, Cholesterol, Diet...                 No   \n",
       "4  Adult, Age Factors, Aged, Arteriosclerosis, Bl...                 No   \n",
       "\n",
       "  Diabetes Primary Study Correct Source Population  \n",
       "0       No           Yes                        EA  \n",
       "1       No           Yes                      SEAP  \n",
       "2       No            No                      SEAP  \n",
       "3       No           Yes                        NA  \n",
       "4       No           Yes                        NA  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = pd.read_excel(excel_path_test, na_values=['na'], keep_default_na=False)\n",
    "input_test.head()\n",
    "input_test.drop(columns=['Source Population'], inplace=True)\n",
    "\n",
    "input_test['Correct Source Population'].fillna('NA', inplace=True)\n",
    "\n",
    "# Combine rows that are similar by PMID into a single row on Correct Source Population\n",
    "input_test = input_test.groupby('PMID').agg({\n",
    "    'Title': 'first',\n",
    "    'Abstract': 'first',\n",
    "    'MeSH': 'first',\n",
    "    'Precision Medicine': 'first',\n",
    "    'Diabetes': 'first',\n",
    "    'Primary Study': 'first',\n",
    "    'Correct Source Population': lambda x: ', '.join(x.unique())\n",
    "}).reset_index()\n",
    "\n",
    "input_test.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2076\n",
      "Number of columns: 8\n"
     ]
    }
   ],
   "source": [
    "rows, columns = input_test.shape\n",
    "print(f\"Number of rows: {rows}\")\n",
    "print(f\"Number of columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_json_splits = 4\n",
    "# Concatenate multiple CSVs\n",
    "results = None\n",
    "for scenario in range(1, n_json_splits + 1):\n",
    "    file_path = f\"{path_equity_precision_llm_folder}/llm training/Test Data Splits/Test Part {scenario}_results.csv\"\n",
    "    temp_df = pd.read_csv(file_path, na_values=['n/a','NaN'], keep_default_na=False)\n",
    "    temp_df = temp_df[['pmid', 'title', 'gpt_precision_medicine', 'gpt_diabetes', 'gpt_primary_study', 'gpt_source_population']]\n",
    "\n",
    "    if results is None:\n",
    "        results = temp_df\n",
    "    else:\n",
    "        results = pd.concat([results, temp_df], ignore_index=True)\n",
    "\n",
    "# Merge datasets\n",
    "merged_df_test = input_test.merge(results, left_on='PMID', right_on='pmid', how='left')\n",
    "\n",
    "\n",
    "\n",
    "merged_df_test['gpt_source_population'] = merged_df_test['gpt_source_population'].apply(standardize_population)\n",
    "merged_df_test = merged_df_test.drop_duplicates(subset='PMID', keep='first').reset_index(drop=True)\n",
    "\n",
    "merged_df_test.to_csv(f\"{path_equity_precision_llm_folder}/llm training/epl06_Test Scenario 4_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2076\n",
      "Number of columns: 14\n"
     ]
    }
   ],
   "source": [
    "rows, columns = merged_df_test.shape\n",
    "print(f\"Number of rows: {rows}\")\n",
    "print(f\"Number of columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in new dataframe: 2196\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file_names = [\n",
    "    f\"{path_equity_precision_llm_folder}/llm training/Training Scenario 4_results.csv\",\n",
    "    f\"{path_equity_precision_llm_folder}/llm training/Development Scenario 4_results.csv\",\n",
    "    f\"{path_equity_precision_llm_folder}/llm training/epl06_Test Scenario 4_results.csv\"\n",
    "]\n",
    "\n",
    "# Columns to keep\n",
    "kept_columns = [\n",
    "    'pmid', 'title', 'gpt_precision_medicine', \n",
    "    'gpt_diabetes', 'gpt_primary_study', 'gpt_source_population'\n",
    "]\n",
    "\n",
    "# Read and concatenate all data into a new df\n",
    "input_all_scenario4 = pd.concat(\n",
    "    [pd.read_csv(file)[kept_columns] for file in file_names],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Check\n",
    "print(f\"Total rows in new dataframe: {input_all_scenario4.shape[0]}\")\n",
    "\n",
    "# Save df\n",
    "output_path = f\"{path_equity_precision_llm_folder}/llm training/input_all_scenario4.csv\"\n",
    "input_all_scenario4.to_csv(output_path, index=False)\n",
    "\n",
    "# Apply standardize_population function\n",
    "input_all_scenario4['gpt_source_population'] = input_all_scenario4['gpt_source_population'].apply(standardize_population)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of studies excluded due to missing Abstract: 1\n",
      "Remaining studies after exclusion: 2195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2195, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the valid source population categories\n",
    "VALID_REGIONS = {\"ca\", \"na\", \"cee\", \"lac\", \"sa\", \"ssa\", \"we\", \"seap\", \"mena\", \"ea\", \"Unknown\"}\n",
    "EUROPEAN_REGIONS = {\"we\", \"na\"}\n",
    "NON_EUROPEAN_REGIONS = VALID_REGIONS - EUROPEAN_REGIONS - {\"Unknown\"}\n",
    "\n",
    "# Step 1: Identify and count excluded studies due to missing Title or Abstract\n",
    "def is_missing(value):\n",
    "    return pd.isna(value) or str(value).strip() == \"\"\n",
    "\n",
    "def excluded(row):\n",
    "    # return is_missing(row['Title'])\n",
    "    return is_missing(row['title']) \n",
    "\n",
    "# Count excluded studies\n",
    "excluded_studies_df = input_all_scenario4[input_all_scenario4.apply(excluded, axis=1)]\n",
    "excluded_count = len(excluded_studies_df)\n",
    "print(f\"Number of studies excluded due to missing Abstract: {excluded_count}\")\n",
    "\n",
    "# Apply exclusion\n",
    "input_all_scenario4_after_exclusion = input_all_scenario4.drop(excluded_studies_df.index)\n",
    "\n",
    "# Verify remaining studies\n",
    "remaining_studies = len(input_all_scenario4_after_exclusion)\n",
    "print(f\"Remaining studies after exclusion: {remaining_studies}\")\n",
    "\n",
    "input_all_scenario4_after_exclusion.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where normalized_regions has more than 1 element: 118\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Normalize source population categories\n",
    "\n",
    "def normalize_source_population(source_population):\n",
    "    if pd.isna(source_population):\n",
    "        return set()\n",
    "    return VALID_REGIONS.intersection(set(re.sub(r'\\s+', '', source_population).split(',')))\n",
    "\n",
    "input_all_scenario4_after_exclusion['normalized_regions'] = input_all_scenario4_after_exclusion['gpt_source_population'].apply(normalize_source_population)\n",
    "\n",
    "# Count rows where normalized_regions has more than 1 element\n",
    "multi_region_rows = input_all_scenario4_after_exclusion[input_all_scenario4_after_exclusion['normalized_regions'].apply(lambda x: len(x) > 1)]\n",
    "multi_region_count = len(multi_region_rows)\n",
    "print(f\"Number of rows where normalized_regions has more than 1 element: {multi_region_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2419, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Expand data **only** for regional counts\n",
    "expanded_rows = []\n",
    "for _, row in input_all_scenario4_after_exclusion.iterrows():\n",
    "    for region in row['normalized_regions']:\n",
    "        new_row = row.copy()\n",
    "        new_row['gpt_source_population'] = region\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Create expanded DataFrame for regional counts\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "expanded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total Precision Medicine': 2195,\n",
       " 'Total Diabetes': 2195,\n",
       " 'Total Primary Study': 2195}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Create a crosstab summary function\n",
    "def crosstab_summary(expanded_df, categories, region_col='gpt_source_population'):\n",
    "    expanded_df['combined_category'] = expanded_df[categories].astype(str).agg('_'.join, axis=1)\n",
    "    summary = pd.crosstab(expanded_df['combined_category'], expanded_df[region_col])\n",
    "    summary.reset_index(inplace=True)\n",
    "    return summary\n",
    "\n",
    "# Define categories\n",
    "categories = ['gpt_precision_medicine', 'gpt_diabetes', 'gpt_primary_study']\n",
    "\n",
    "# Generate crosstab for regional counts\n",
    "summary_expanded_df = crosstab_summary(expanded_df, categories, region_col='gpt_source_population')\n",
    "\n",
    "# Step 5: Compute overall totals without double-counting studies\n",
    "total_counts = {\n",
    "    'Total Precision Medicine': input_all_scenario4_after_exclusion['gpt_precision_medicine'].notna().sum(),\n",
    "    'Total Diabetes': input_all_scenario4_after_exclusion['gpt_diabetes'].notna().sum(),\n",
    "    'Total Primary Study': input_all_scenario4_after_exclusion['gpt_primary_study'].notna().sum()\n",
    "}\n",
    "\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>gpt_source_population</th>\n",
       "      <th>combined_category</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>ca</th>\n",
       "      <th>cee</th>\n",
       "      <th>ea</th>\n",
       "      <th>lac</th>\n",
       "      <th>mena</th>\n",
       "      <th>na</th>\n",
       "      <th>sa</th>\n",
       "      <th>seap</th>\n",
       "      <th>ssa</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_no_no</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_no_yes</td>\n",
       "      <td>214</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_yes_no</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_yes_not determined</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no_yes_yes</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>88</td>\n",
       "      <td>52</td>\n",
       "      <td>36</td>\n",
       "      <td>56</td>\n",
       "      <td>50</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes_no_no</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes_no_unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yes_no_yes</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yes_yes_no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yes_yes_not determinable</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yes_yes_yes</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>206</td>\n",
       "      <td>187</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "      <td>165</td>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "gpt_source_population         combined_category  Unknown  ca  cee   ea  lac  \\\n",
       "0                                      no_no_no       79   1    1    9    7   \n",
       "1                                     no_no_yes      214   6   51   70   67   \n",
       "2                                     no_yes_no        1   2    5   13    1   \n",
       "3                         no_yes_not determined        1   0    0    0    0   \n",
       "4                                    no_yes_yes       48   3   30   88   52   \n",
       "5                                     yes_no_no        3   0    0    1    3   \n",
       "6                                yes_no_unknown        0   0    0    0    1   \n",
       "7                                    yes_no_yes       16   0   13   67   67   \n",
       "8                                    yes_yes_no        0   0    1    9   10   \n",
       "9                      yes_yes_not determinable        0   0    0    0    0   \n",
       "10                                  yes_yes_yes       19   2   42  206  187   \n",
       "\n",
       "gpt_source_population  mena   na   sa  seap  ssa  we  \n",
       "0                         2   19    5     1    4  10  \n",
       "1                        18   50   26    20    9  25  \n",
       "2                         1   11   15     3    1   9  \n",
       "3                         0    0    0     0    0   0  \n",
       "4                        36   56   50    35   16  30  \n",
       "5                         0    5    4     0    0   4  \n",
       "6                         0    0    0     0    0   0  \n",
       "7                         7   34   31    10    4  12  \n",
       "8                         0   12   13     3    2   4  \n",
       "9                         0    0    1     0    0   0  \n",
       "10                       17  134  165    36   21  52  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Save the expanded output for regions\n",
    "output_path = path_equity_precision_llm_repo + '/preprocessing/epl06_manually screened.csv'\n",
    "summary_expanded_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display summary\n",
    "display(summary_expanded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Categorize studies into European vs. Non-European vs. Unknown (without duplicates)\n",
    "def classify_region(regions):\n",
    "    classification = set()\n",
    "    if any(r in EUROPEAN_REGIONS for r in regions):\n",
    "        classification.add('European')\n",
    "    if any(r in NON_EUROPEAN_REGIONS for r in regions):\n",
    "        classification.add('Non-European')\n",
    "    if not classification:\n",
    "        classification.add('Unknown')\n",
    "    return classification\n",
    "\n",
    "# Remove rows where gpt_source_population is 'Unknown'\n",
    "\n",
    "\n",
    "input_all_scenario4_after_exclusion['classification_group'] = input_all_scenario4_after_exclusion['normalized_regions'].apply(classify_region)\n",
    "\n",
    "input_all_scenario4_after_exclusion_region = input_all_scenario4_after_exclusion[input_all_scenario4_after_exclusion['gpt_source_population'] != 'Unknown']\n",
    "\n",
    "# Compute total counts per classification group\n",
    "group_totals = input_all_scenario4_after_exclusion_region['classification_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expanded_rows_region = []\n",
    "for _, row in input_all_scenario4_after_exclusion_region.iterrows():\n",
    "    for group in row['classification_group']:\n",
    "        new_row = row.copy()\n",
    "        new_row['classification_group'] = group\n",
    "        expanded_rows_region.append(new_row)\n",
    "\n",
    "# Create expanded DataFrame for regional counts\n",
    "expanded_df_region = pd.DataFrame(expanded_rows_region)\n",
    "expanded_df_region.shape\n",
    "\n",
    "\n",
    "summary_expanded_df_region = crosstab_summary(expanded_df_region, categories, region_col='classification_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>classification_group</th>\n",
       "      <th>combined_category</th>\n",
       "      <th>European</th>\n",
       "      <th>Non-European</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_no_no</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_no_yes</td>\n",
       "      <td>74</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_yes_no</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_yes_yes</td>\n",
       "      <td>83</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes_no_no</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes_no_unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes_no_yes</td>\n",
       "      <td>42</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yes_yes_no</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yes_yes_not determinable</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yes_yes_yes</td>\n",
       "      <td>173</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "classification_group         combined_category  European  Non-European\n",
       "0                                     no_no_no        24            25\n",
       "1                                    no_no_yes        74           257\n",
       "2                                    no_yes_no        18            33\n",
       "3                                   no_yes_yes        83           297\n",
       "4                                    yes_no_no         7             8\n",
       "5                               yes_no_unknown         0             1\n",
       "6                                   yes_no_yes        42           191\n",
       "7                                   yes_yes_no        12            27\n",
       "8                     yes_yes_not determinable         0             1\n",
       "9                                  yes_yes_yes       173           631"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 8: Save the expanded output for regions\n",
    "output_path_group = path_equity_precision_llm_repo + '/preprocessing/epl06_manually screened group.csv'\n",
    "summary_expanded_df_region.to_csv(output_path_group, index=False)\n",
    "\n",
    "# Display summary\n",
    "display(summary_expanded_df_region)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
