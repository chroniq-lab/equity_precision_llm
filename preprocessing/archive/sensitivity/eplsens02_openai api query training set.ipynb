{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to access the OpenAI application programming interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your command prompt window:\n",
    ">> pip install openai\n",
    "\n",
    "https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kneed in c:\\users\\jvargh7\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.5)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.14.2 in c:\\users\\jvargh7\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kneed) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\jvargh7\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kneed) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn as sklearn\n",
    "import os as os\n",
    "import getpass\n",
    "from openai import OpenAI, APIConnectionError\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re as re\n",
    "import datetime as datetime\n",
    "# Ensure all necessary packages are installed\n",
    "%pip install kneed\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "if user == \"JVARGH7\":\n",
    "    path_equity_precision_llm_folder = \"C:/Cloud/OneDrive - Emory University/Papers/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "    path_equity_precision_llm_repo =  'C:/code/external/equity_precision_llm'\n",
    "\n",
    "elif user == \"sophiakim\":\n",
    "    path_equity_precision_llm_folder = \"/Users/sophiakim/Library/CloudStorage/OneDrive-SharedLibraries-Emory/Varghese, Jithin Sam - Global Equity in Diabetes Precision Medicine LLM\"\n",
    "    path_equity_precision_llm_repo = '/Users/sophiakim/equity_precision_llm'\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unrecognized user\")\n",
    "\n",
    "excel_path = path_equity_precision_llm_folder + \"/o3-mini training/Training Data.xlsx\"\n",
    "# path_equity_precision_llm_repo = os.path.abspath(\"\").replace(\"preprocessing\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_epl_shared = \"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "# https://stackoverflow.com/questions/36959031/how-to-source-file-into-python-script\n",
    "execfile(path_equity_precision_llm_repo + \"/constants.py\")\n",
    "\n",
    "if user == \"JVARGH7\":\n",
    "    api_key_epl_shared = rc_shared_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to outline inclusion criteria for three categories: diabetes, precision medicine, and primary study. \n",
      "\n",
      "Please wait for me to prompt you on what to do based on these criteria.  \n",
      "Here are the inclusion criteria:  \n",
      "\n",
      "PRECISION MEDICINE: Precision medicine is an assessment of genetic or metabolic state to guide preventive and therapeutic decisions in humans â€“ including gaining more understanding. Exclude epidemiological studies using traditional biomarkers only, focusing on omics (genomics, metabolomics, proteomics, lipidomics etc.) or multi-omics studies.  Include genome-wide linkage analysis and genetic analysesâ€”including phylogenetic analysis and genotype-phenotype association. Including alleles, human genome, and genes.\n",
      "\n",
      "DIABETES: Do not exclude any type of diabetes or prediabetes. The presence of certain conditions or risk factors may not definitively confirm that the study is related to diabetes or prediabetes, unless there is a clear link to diabetes pathophysiology, diagnosis, or management. If the paper doesnâ€™t state diabetes directly, classify it as \"no\"\n",
      "\n",
      "PRIMARY STUDY: All types of studies, other than meta-analysis and systematic reviews, were evaluated as primary studies. Although it may bias the studies towards counting contributions of large biorepositories, secondary analysis of previously collected datasets was also included since these generate novel insights into the pathophysiology of diabetes for source populations that these biorepositories sample from. \n",
      "\n",
      "Based on the PMID, title, abstract, and MeSH terms below and the previously described inclusion criteria, identify using 'yes' or 'no' as categorizations if the article fulfills the criteria for precision medicine, diabetes, and primary study. \n",
      "\n",
      "PMID: 22744164\n",
      "\n",
      "Title: Acculturation and glycemic control of Asian Indian adults with type 2 diabetes \n",
      "\n",
      "Abstract: The prevalence of type 2 diabetes is disproportionately high among Asian Indians (AI), one of the fastest growing immigrant groups in the United States (US). Poorly controlled diabetes associated with inadequate self-management increases complications and thus medical costs. Acculturation may be an important determinant of diabetes self-management and hence control. This study examined the association between the degree of acculturation and glycemic control as measured by Hemoglobin A1c in AI adults with type 2 diabetes. A mixed method (quantitative and qualitative) study was conducted among 30 AI adults with type 2 diabetes. Acculturation assessment using the Suinn-Lew Asian Self-identity Instrument was followed by socio-demographic questions, self-reported anthropometric measures, and open ended diabetes self-care questions. A two-step multiple regression analysis and content analysis of verbatim interview transcriptions were conducted. Interactions of acculturation with body mass index (interaction b = 1.11; p = 0.01), annual household income (interaction b = 7.19; p = .01), and diabetes duration (interaction b = .30; p = .02) significantly predicted higher HbA1c levels (R(2) change = .368; F change = 4.21; p = .02). From the qualitative interviews, the following were regarded as US specific facilitators for glycemic control: excellent health care system and facilities, availability of healthy food choices and self-monitoring devices, medical insurance benefits, good quality medications, and improved health awareness. Cultural orientation might be important for patient tailored interventions targeting AI with type 2 diabetes. Therefore, interventions targeted at Asian Indians with diabetes should include culture specific adaptations to nutrition education and support.    \n",
      "\n",
      "MeSH: Acculturation*; Asian / psychology; Asian / statistics & numerical data*; Body Mass Index; Diabetes Mellitus, Type 2 / ethnology; Diabetes Mellitus, Type 2 / therapy*; Diet / statistics & numerical data; Female; Glycated Hemoglobin / analysis; Humans; Income / statistics & numerical data; India / ethnology; Male; Middle Aged; Self Care / statistics & numerical data; United States   \n",
      "\n",
      "Organize your analysis of this abstract in a table, with the following columns: PMID, title, precision medicine, diabetes, primary study. Please provide evidence directly from the PMID, title, abstract, and MeSH terms to justify your classifications. \n",
      "Now identify the source population. The correct source population is based on the study participants' racial/ethnic background, regardless of geographic location, and refers to humans specifically. Exclude plants and therapeutics that may originate in the region and non-human animal models. The options are humans descending from Central Asia (CA), Central and Eastern Europe (CEE), East Asia (EA), Latin America & Caribbean (LAC), Middle East & North Africa (MENA), South East Asia & Pacific Islands (SEAP), South Asia (SA), Sub-Saharan Africa (SSA), the United States of America and Canada (NA), or Western Europe (WE). \n",
      "\n",
      "Add the classification of the source population to the table in a separate column. You may only use the abbreviation of the source populations provided. \n"
     ]
    }
   ],
   "source": [
    "execfile(path_equity_precision_llm_repo + \"/functions/prompt_generator.py\")\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/base_prompt_append.py\")\n",
    "\n",
    "base_prompt_files = ['s1v6', 's2v6', 's3v6','s4v6']\n",
    "base_prompts = base_prompt_append(base_prompt_files)\n",
    "\n",
    "prompt_pmid = prompt_generator(22744164, base_prompts, excel_path,sheet_name = 'Training Data')\n",
    "\n",
    "\n",
    "print(base_prompts[0])\n",
    "print(base_prompts[1])\n",
    "print(prompt_pmid)\n",
    "print(base_prompts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing Your Batch File\n",
    "\n",
    "https://platform.openai.com/docs/guides/batch\n",
    "\n",
    ".jsonl file where each line contains the details of an individual request to the API\n",
    "\n",
    "Each request must include a custom_id value\n",
    "\n",
    "{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n",
    "\n",
    "\n",
    "{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an unhelpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_list = pd.read_excel(excel_path, sheet_name='Training Data')['PMID'].tolist()\n",
    "\n",
    "json_list_scenario1 = []\n",
    "json_list_scenario2 = []\n",
    "json_list_scenario3 = []\n",
    "json_list_scenario4 = []\n",
    "\n",
    "for index, pmid in enumerate(pmid_list):\n",
    "    prompt_pmid = prompt_generator(pmid, base_prompts, excel_path,sheet_name = 'Training Data')\n",
    "    dict_pmid_scenario1 = {\"custom_id\": str(index) + \"_\" + str(pmid), \n",
    "                 \"method\": \"POST\", \n",
    "                 \"url\": \"/v1/chat/completions\", \n",
    "                 \"body\": {\"model\": \"gpt-o3-mini-high\", \n",
    "                          \"messages\": [ {\"role\": \"system\", \"content\": base_prompts[0]},\n",
    "                                        {\"role\": \"system\", \"content\": base_prompts[1]},\n",
    "                                        {\"role\": \"user\", \"content\": prompt_pmid},\n",
    "                                        {\"role\": \"user\", \"content\": base_prompts[3]}],\n",
    "                            \"max_tokens\": 1000,\n",
    "                            \"frequency_penalty\": 0.2,\n",
    "                            \"temperature\": 0.2,\n",
    "                            \"presence_penalty\": 0.3\n",
    "\n",
    "                        }\n",
    "                }\n",
    "    \n",
    "    dict_pmid_scenario2 = {\"custom_id\": str(index) + \"_\" + str(pmid), \n",
    "                 \"method\": \"POST\", \n",
    "                 \"url\": \"/v1/chat/completions\", \n",
    "                 \"body\": {\"model\": \"gpt-o3-mini-high\", \n",
    "                          \"messages\": [ {\"role\": \"system\", \"content\": base_prompts[0]},\n",
    "                                        {\"role\": \"system\", \"content\": base_prompts[1]},\n",
    "                                        {\"role\": \"user\", \"content\": prompt_pmid},\n",
    "                                        {\"role\": \"user\", \"content\": base_prompts[3]}],\n",
    "                            \"max_tokens\": 1000,\n",
    "                            \"frequency_penalty\": -0.2,\n",
    "                            \"temperature\": 0.2,\n",
    "                            \"presence_penalty\": -0.3\n",
    "\n",
    "                        }\n",
    "                }\n",
    "    \n",
    "    dict_pmid_scenario3 = {\"custom_id\": str(index) + \"_\" + str(pmid), \n",
    "                 \"method\": \"POST\", \n",
    "                 \"url\": \"/v1/chat/completions\", \n",
    "                 \"body\": {\"model\": \"gpt-o3-mini-high\", \n",
    "                          \"messages\": [ {\"role\": \"system\", \"content\": base_prompts[0]},\n",
    "                                        {\"role\": \"system\", \"content\": base_prompts[1]},\n",
    "                                        {\"role\": \"user\", \"content\": prompt_pmid},\n",
    "                                        {\"role\": \"user\", \"content\": base_prompts[3]}],\n",
    "                            \"max_tokens\": 1000,\n",
    "                            \"frequency_penalty\": 0.2,\n",
    "                            \"top_p\": 0.8,\n",
    "                            \"presence_penalty\": 0.3\n",
    "\n",
    "                        }\n",
    "                }\n",
    "\n",
    "    dict_pmid_scenario4 = {\"custom_id\": str(index) + \"_\" + str(pmid), \n",
    "                 \"method\": \"POST\", \n",
    "                 \"url\": \"/v1/chat/completions\", \n",
    "                 \"body\": {\"model\": \"gpt-o3-mini-high\", \n",
    "                          \"messages\": [ {\"role\": \"system\", \"content\": base_prompts[0]},\n",
    "                                        {\"role\": \"system\", \"content\": base_prompts[1]},\n",
    "                                        {\"role\": \"user\", \"content\": prompt_pmid},\n",
    "                                        {\"role\": \"user\", \"content\": base_prompts[3]}],\n",
    "                            \"max_tokens\": 1000,\n",
    "                            \"frequency_penalty\": -0.2,\n",
    "                            \"top_p\": 0.8,\n",
    "                            \"presence_penalty\": -0.3\n",
    "\n",
    "                        }\n",
    "                }\n",
    "    json_list_scenario1.append(dict_pmid_scenario1)\n",
    "    \n",
    "    json_list_scenario2.append(dict_pmid_scenario2)\n",
    "\n",
    "    json_list_scenario3.append(dict_pmid_scenario3)\n",
    "\n",
    "    json_list_scenario4.append(dict_pmid_scenario4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(path_equity_precision_llm_folder + '/o3-mini training/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: '/Users/sophiakim/Library/CloudStorage/OneDrive-SharedLibraries-Emory/Varghese, Jithin Sam - Global Equity in Diabetes Precision Medicine LLM/o3-mini training/Sensitivity Scenario 1.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_equity_precision_llm_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/o3-mini training/Sensitivity Scenario 1.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m json_list_scenario1:\n\u001b[1;32m      4\u001b[0m         json_line \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(entry)\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/Users/sophiakim/Library/CloudStorage/OneDrive-SharedLibraries-Emory/Varghese, Jithin Sam - Global Equity in Diabetes Precision Medicine LLM/o3-mini training/Sensitivity Scenario 1.jsonl'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(path_equity_precision_llm_folder + '/o3-mini training/Sensitivity Scenario 1.jsonl', 'w') as outfile:\n",
    "    for entry in json_list_scenario1:\n",
    "        json_line = json.dumps(entry)\n",
    "        outfile.write(json_line + '\\n')\n",
    "\n",
    "with open(path_equity_precision_llm_folder + '/o3-mini training/Sensitivity Scenario 2.jsonl', 'w') as outfile:\n",
    "    for entry in json_list_scenario2:\n",
    "        json_line = json.dumps(entry)\n",
    "        outfile.write(json_line + '\\n')\n",
    "\n",
    "with open(path_equity_precision_llm_folder + '/o3-mini training/Sensitivity Scenario 3.jsonl', 'w') as outfile:\n",
    "    for entry in json_list_scenario3:\n",
    "        json_line = json.dumps(entry)\n",
    "        outfile.write(json_line + '\\n')\n",
    "\n",
    "with open(path_equity_precision_llm_folder + '/o3-mini training/Sensitivity Scenario 4.jsonl', 'w') as outfile:\n",
    "    for entry in json_list_scenario4:\n",
    "        json_line = json.dumps(entry)\n",
    "        outfile.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Uploading Your Batch Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\o'\n",
      "/var/folders/7f/_w1wqc_922vdwts333pkdz040000gn/T/ipykernel_13729/3986250925.py:5: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  file=open(path_equity_precision_llm_folder + '\\o3-mini training\\Training Scenario 1.jsonl', \"rb\"),\n",
      "/var/folders/7f/_w1wqc_922vdwts333pkdz040000gn/T/ipykernel_13729/3986250925.py:10: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  file=open(path_equity_precision_llm_folder + '\\o3-mini training\\Training Scenario 2.jsonl', \"rb\"),\n",
      "/var/folders/7f/_w1wqc_922vdwts333pkdz040000gn/T/ipykernel_13729/3986250925.py:15: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  file=open(path_equity_precision_llm_folder + '\\o3-mini training\\Training Scenario 3.jsonl', \"rb\"),\n",
      "/var/folders/7f/_w1wqc_922vdwts333pkdz040000gn/T/ipykernel_13729/3986250925.py:20: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  file=open(path_equity_precision_llm_folder + '\\o3-mini training\\Training Scenario 4.jsonl', \"rb\"),\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "client = OpenAI(api_key= api_key_epl_shared)\n",
    "\n",
    "try:\n",
    "    batch_input_file_scenario1 = client.files.create(\n",
    "      file=open(path_equity_precision_llm_folder + '/o3-mini training/Sensitivity Scenario 1.jsonl', \"rb\"),\n",
    "      purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_scenario2 = client.files.create(\n",
    "      file=open(path_equity_precision_llm_folder + '/o3-mini training/Sensitivity Scenario 2.jsonl', \"rb\"),\n",
    "      purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_scenario3 = client.files.create(\n",
    "      file=open(path_equity_precision_llm_folder + '/o3-mini training/Sensitivity Scenario 3.jsonl', \"rb\"),\n",
    "      purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_scenario4 = client.files.create(\n",
    "      file=open(path_equity_precision_llm_folder + '/o3-mini training/Sensitivity Scenario 4.jsonl', \"rb\"),\n",
    "      purpose=\"batch\"\n",
    "    )\n",
    "except APIConnectionError as e:\n",
    "    print(f\"Connection error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-AcphAJFB25rb7byuJDqLSn\n",
      "file-AcphAJFB25rb7byuJDqLSn\n",
      "file-YXK4kDwKWtxhq9q36AN8Me\n",
      "file-YFMFiEV3myg6j61pMVBQb3\n"
     ]
    }
   ],
   "source": [
    "batch_input_file_scenario1_id = batch_input_file_scenario1.id\n",
    "print(batch_input_file_scenario1_id)\n",
    "batch_created_scenario1 = client.batches.create(\n",
    "    input_file_id=batch_input_file_scenario1_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"sensitivity data for PMID query - scenario 1\"\n",
    "    }\n",
    ")\n",
    "\n",
    "batch_input_file_scenario2_id = batch_input_file_scenario2.id\n",
    "print(batch_input_file_scenario1_id)\n",
    "batch_created_scenario2 = client.batches.create(\n",
    "    input_file_id=batch_input_file_scenario2_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"sensitivity data for PMID query - scenario 2\"\n",
    "    }\n",
    ")\n",
    "\n",
    "batch_input_file_scenario3_id = batch_input_file_scenario3.id\n",
    "print(batch_input_file_scenario3_id)\n",
    "batch_created_scenario3 = client.batches.create(\n",
    "    input_file_id=batch_input_file_scenario3_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"sensitivity data for PMID query - scenario 3\"\n",
    "    }\n",
    ")\n",
    "\n",
    "batch_input_file_scenario4_id = batch_input_file_scenario4.id\n",
    "print(batch_input_file_scenario4_id)\n",
    "batch_created_scenario4 = client.batches.create(\n",
    "    input_file_id=batch_input_file_scenario4_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"sensitivity data for PMID query - scenario 4\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_file_path = path_equity_precision_llm_repo + \"/Batches.txt\"\n",
    "\n",
    "if not os.path.exists(batches_file_path):\n",
    "    with open(batches_file_path, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "with open(batches_file_path, \"a\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    current_time = datetime.datetime.now()\n",
    "    f.write(f\"# {current_time}\\n\")\n",
    "    f.write(f\"batch_created_scenario1_id = '{batch_created_scenario1.id}'\\n\")\n",
    "    f.write(f\"batch_created_scenario2_id = '{batch_created_scenario2.id}'\\n\")\n",
    "    f.write(f\"batch_created_scenario3_id = '{batch_created_scenario3.id}'\\n\")\n",
    "    f.write(f\"batch_created_scenario4_id = '{batch_created_scenario4.id}'\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_created_scenario1_id = batch_created_scenario1.id\n",
    "# batch_created_scenario2_id = batch_created_scenario2.id\n",
    "# batch_created_scenario3_id = batch_created_scenario3.id\n",
    "# batch_created_scenario4_id = batch_created_scenario4.id\n",
    "\n",
    "\n",
    "batch_created_scenario1_id = 'batch_680012404a188190a691d6b3e0a334c9'\n",
    "batch_created_scenario2_id = 'batch_6800124075308190ad464920a414bee3'\n",
    "batch_created_scenario3_id = 'batch_68001240bba081908bca0c17e734f187'\n",
    "batch_created_scenario4_id = 'batch_68001240e7308190bef36f9f39ce76be'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Checking the Status of a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1: completed; Scenario 2: completed; Scenario 3:completed; Scenario 4:completed\n"
     ]
    }
   ],
   "source": [
    "# batch_created_id = 'batch_675d9dc3f25881909544964060c659c3'\n",
    "client = OpenAI(api_key= api_key_epl_shared)\n",
    "\n",
    "batch_status_scenario1 = client.batches.retrieve(batch_created_scenario1_id)\n",
    "batch_status_scenario2 = client.batches.retrieve(batch_created_scenario2_id)\n",
    "batch_status_scenario3 = client.batches.retrieve(batch_created_scenario3_id)\n",
    "\n",
    "batch_status_scenario4 = client.batches.retrieve(batch_created_scenario4_id)\n",
    "\n",
    "print(\"Scenario 1: \" + batch_status_scenario1.status + \"; Scenario 2: \" + batch_status_scenario2.status + \"; Scenario 3:\" + batch_status_scenario3.status + \"; Scenario 4:\" + batch_status_scenario4.status)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retrieving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key= api_key_epl_shared)\n",
    "\n",
    "file_response_scenario1 = client.files.content(batch_status_scenario1.output_file_id)\n",
    "file_response_scenario2 = client.files.content(batch_status_scenario2.output_file_id)\n",
    "file_response_scenario3 = client.files.content(batch_status_scenario3.output_file_id)\n",
    "file_response_scenario4 = client.files.content(batch_status_scenario4.output_file_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:/code/external/equity_precision_llm/functions/format_o3_output.py:6: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_json(json_output.content.decode('utf-8'), lines=True)\n",
      "C:/code/external/equity_precision_llm/functions/format_o3_output.py:6: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_json(json_output.content.decode('utf-8'), lines=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m results1\u001b[38;5;241m.\u001b[39mto_csv(path_equity_precision_llm_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mo3-mini training\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTraining Scenario 1_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m results2\u001b[38;5;241m.\u001b[39mto_csv(path_equity_precision_llm_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mo3-mini training\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTraining Scenario 2_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mresults3\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(path_equity_precision_llm_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mo3-mini training\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTraining Scenario 3_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m results4\u001b[38;5;241m.\u001b[39mto_csv(path_equity_precision_llm_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mo3-mini training\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTraining Scenario 4_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results3' is not defined"
     ]
    }
   ],
   "source": [
    "execfile(path_equity_precision_llm_repo + \"/functions/format_o3_output.py\")\n",
    "\n",
    "results1 = format_o3_output(file_response_scenario1)\n",
    "results2 = format_o3_output(file_response_scenario2)\n",
    "# results3 = format_o3_output(file_response_scenario3)\n",
    "# results4 = format_o3_output(file_response_scenario4)\n",
    "\n",
    "\n",
    "\n",
    "results1.to_csv(path_equity_precision_llm_folder + '\\o3-mini training\\Training Scenario 1_results.csv', index=False)\n",
    "results2.to_csv(path_equity_precision_llm_folder + '\\o3-mini training\\Training Scenario 2_results.csv', index=False)\n",
    "# results3.to_csv(path_equity_precision_llm_folder + '\\o3-mini training\\Training Scenario 3_results.csv', index=False)\n",
    "# results4.to_csv(path_equity_precision_llm_folder + '\\o3-mini training\\Training Scenario 4_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>title</th>\n",
       "      <th>gpt_precision_medicine</th>\n",
       "      <th>gpt_diabetes</th>\n",
       "      <th>gpt_primary_study</th>\n",
       "      <th>gpt_source_population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22744164</td>\n",
       "      <td>Acculturation and glycemic control of Asian I...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>sa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15561964</td>\n",
       "      <td>Linkage analysis of diabetes status among hyp...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28770629</td>\n",
       "      <td>Ipragliflozin, a sodium glucose co-transporte...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>ea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33764184</td>\n",
       "      <td>Gastrodin protects against high glucose-induc...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36155119</td>\n",
       "      <td>Curcumin supplementation reduces blood glucos...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>lac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35477306</td>\n",
       "      <td>Gut Microbiome Alterations Associated with Di...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>lac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23153210</td>\n",
       "      <td>Mammalian NPC1 genes may undergo positive sel...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>mena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34794279</td>\n",
       "      <td>Relationship between Serum Elabela Level and ...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>cee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12116231</td>\n",
       "      <td>Candidate genes involved in cardiovascular ri...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>seap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2109332</td>\n",
       "      <td>Rural/urban differences of diabetes--impaired...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>seap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6453040</td>\n",
       "      <td>[Gd- allele distribution patterns in Azerbaij...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>cee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31807629</td>\n",
       "      <td>Genetic variations in the sheep SIRT7 gene an...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>28068033</td>\n",
       "      <td>Dietary gender differences in terms of the ri...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>cee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>28668296</td>\n",
       "      <td>IgG glycan patterns are associated with type ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cee, we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34872002</td>\n",
       "      <td>SOX9-mediated UGT8 expression promotes glycol...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32663752</td>\n",
       "      <td>Hypoglycemic and hypolipidemic effects of Mor...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>not specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12475769</td>\n",
       "      <td>Evidence for a novel type 1 diabetes suscepti...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22113416</td>\n",
       "      <td>Single nucleotide polymorphisms in JAZF1 and ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12638530</td>\n",
       "      <td>Long-term evolution of blood lipid profiles a...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>36649380</td>\n",
       "      <td>A novel splice-affecting HNF1A variant with l...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>seap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pmid                                              title  \\\n",
       "0   22744164   Acculturation and glycemic control of Asian I...   \n",
       "1   15561964   Linkage analysis of diabetes status among hyp...   \n",
       "2   28770629   Ipragliflozin, a sodium glucose co-transporte...   \n",
       "3   33764184   Gastrodin protects against high glucose-induc...   \n",
       "4   36155119   Curcumin supplementation reduces blood glucos...   \n",
       "5   35477306   Gut Microbiome Alterations Associated with Di...   \n",
       "6   23153210   Mammalian NPC1 genes may undergo positive sel...   \n",
       "7   34794279   Relationship between Serum Elabela Level and ...   \n",
       "8   12116231   Candidate genes involved in cardiovascular ri...   \n",
       "9    2109332   Rural/urban differences of diabetes--impaired...   \n",
       "10   6453040   [Gd- allele distribution patterns in Azerbaij...   \n",
       "11  31807629   Genetic variations in the sheep SIRT7 gene an...   \n",
       "12  28068033   Dietary gender differences in terms of the ri...   \n",
       "13  28668296   IgG glycan patterns are associated with type ...   \n",
       "14  34872002   SOX9-mediated UGT8 expression promotes glycol...   \n",
       "15  32663752   Hypoglycemic and hypolipidemic effects of Mor...   \n",
       "16  12475769   Evidence for a novel type 1 diabetes suscepti...   \n",
       "17  22113416   Single nucleotide polymorphisms in JAZF1 and ...   \n",
       "18  12638530   Long-term evolution of blood lipid profiles a...   \n",
       "19  36649380   A novel splice-affecting HNF1A variant with l...   \n",
       "\n",
       "   gpt_precision_medicine gpt_diabetes gpt_primary_study gpt_source_population  \n",
       "0                      no          yes               yes                    sa  \n",
       "1                     yes          yes               yes                    na  \n",
       "2                      no          yes               yes                    ea  \n",
       "3                      no          yes               yes                   n/a  \n",
       "4                      no          yes               yes                   lac  \n",
       "5                     yes          yes               yes                   lac  \n",
       "6                     yes          yes               yes                  mena  \n",
       "7                      no           no               yes                   cee  \n",
       "8                     yes          yes               yes                  seap  \n",
       "9                      no          yes               yes                  seap  \n",
       "10                     no           no               yes                   cee  \n",
       "11                     no           no               yes                   n/a  \n",
       "12                     no           no               yes                   cee  \n",
       "13                    yes          yes               yes               cee, we  \n",
       "14                     no           no               yes                    na  \n",
       "15                     no          yes               yes         not specified  \n",
       "16                    yes          yes               yes                    na  \n",
       "17                    yes          yes               yes                    na  \n",
       "18                     no           no               yes                    we  \n",
       "19                    yes          yes               yes                  seap  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>title</th>\n",
       "      <th>gpt_precision_medicine</th>\n",
       "      <th>gpt_diabetes</th>\n",
       "      <th>gpt_primary_study</th>\n",
       "      <th>gpt_source_population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22744164</td>\n",
       "      <td>Acculturation and glycemic control of Asian I...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>sa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pmid                                              title  \\\n",
       "0  22744164   Acculturation and glycemic control of Asian I...   \n",
       "\n",
       "  gpt_precision_medicine gpt_diabetes gpt_primary_study gpt_source_population  \n",
       "0                     no          yes               yes                    sa  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results4.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
