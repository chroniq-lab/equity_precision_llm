{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to access the OpenAI application programming interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your command prompt window:\n",
    ">> pip install openai\n",
    "\n",
    "https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kneed in c:\\users\\jvargh7\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: numpy>=1.14.2 in c:\\users\\jvargh7\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kneed) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\jvargh7\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kneed) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn as sklearn\n",
    "import os as os\n",
    "import getpass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re as re\n",
    "import datetime as datetime\n",
    "# Ensure all necessary packages are installed\n",
    "%pip install kneed\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "if user == \"JVARGH7\":\n",
    "    path_equity_precision_llm_folder = \"C:/Cloud/OneDrive - Emory University/Papers/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "    path_equity_precision_llm_repo =  'C:/code/external/equity_precision_llm'\n",
    "\n",
    "elif user == \"aamnasoniwala\":\n",
    "    path_equity_precision_llm_folder = \"/Users/aamnasoniwala/Library/CloudStorage/OneDrive-Emory/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "    path_equity_precision_llm_repo = '/Users/aamnasoniwala/Desktop/equity_precision_llm'\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unrecognized user\")\n",
    "\n",
    "excel_path = path_equity_precision_llm_folder + \"/llm training/Methods.xlsx\"\n",
    "# path_equity_precision_llm_repo = os.path.abspath(\"\").replace(\"preprocessing\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_epl_shared = \"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "# https://stackoverflow.com/questions/36959031/how-to-source-file-into-python-script\n",
    "execfile(path_equity_precision_llm_repo + \"/constants.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to outline inclusion criteria for three categories: diabetes, precision medicine, and primary study. \n",
      "\n",
      "Please wait for me to prompt you on what to do based on these criteria.  \n",
      "Here are the inclusion criteria:  \n",
      "\n",
      "DIABETES: Do not exclude any type of diabetes or prediabetes. The presence of certain conditions or risk factors may not definitively confirm that the study is related to diabetes or prediabetes, unless there is a clear link to diabetes pathophysiology, diagnosis, or management. \n",
      "\n",
      "PRECISION MEDICINE: Precision medicine is an assessment of genetic or metabolic state to guide preventive and therapeutic decisions in humans. Exclude epidemiological studies using traditional biomarkers only, focusing on omics (genomics, metabolomics, proteomics, lipidomics etc.) or multi-omics studies. \n",
      "\n",
      "PRIMARY STUDY: All types of studies, other than meta-analysis and systematic reviews, were evaluated as primary studies. Although it may bias the studies towards counting contributions of large biorepositories, secondary analysis of previously collected datasets was also included since these generate novel insights into the pathophysiology of diabetes for source populations that these biorepositories sample from.  \n",
      "Based on the PMID, title, abstract, and MeSH terms below and the previously described inclusion criteria, identify using 'yes' or 'no' as categorizations if the article fulfills the criteria for precision medicine, diabetes, and primary study. \n",
      "\n",
      "PMID: 22744164\n",
      "\n",
      "Title: Acculturation and glycemic control of Asian Indian adults with type 2 diabetes \n",
      "\n",
      "Abstract: The prevalence of type 2 diabetes is disproportionately high among Asian Indians (AI), one of the fastest growing immigrant groups in the United States (US). Poorly controlled diabetes associated with inadequate self-management increases complications and thus medical costs. Acculturation may be an important determinant of diabetes self-management and hence control. This study examined the association between the degree of acculturation and glycemic control as measured by Hemoglobin A1c in AI adults with type 2 diabetes. A mixed method (quantitative and qualitative) study was conducted among 30 AI adults with type 2 diabetes. Acculturation assessment using the Suinn-Lew Asian Self-identity Instrument was followed by socio-demographic questions, self-reported anthropometric measures, and open ended diabetes self-care questions. A two-step multiple regression analysis and content analysis of verbatim interview transcriptions were conducted. Interactions of acculturation with body mass index (interaction b = 1.11; p = 0.01), annual household income (interaction b = 7.19; p = .01), and diabetes duration (interaction b = .30; p = .02) significantly predicted higher HbA1c levels (R(2) change = .368; F change = 4.21; p = .02). From the qualitative interviews, the following were regarded as US specific facilitators for glycemic control: excellent health care system and facilities, availability of healthy food choices and self-monitoring devices, medical insurance benefits, good quality medications, and improved health awareness. Cultural orientation might be important for patient tailored interventions targeting AI with type 2 diabetes. Therefore, interventions targeted at Asian Indians with diabetes should include culture specific adaptations to nutrition education and support.    \n",
      "\n",
      "MeSH: Acculturation*; Asian / psychology; Asian / statistics & numerical data*; Body Mass Index; Diabetes Mellitus, Type 2 / ethnology; Diabetes Mellitus, Type 2 / therapy*; Diet / statistics & numerical data; Female; Glycated Hemoglobin / analysis; Humans; Income / statistics & numerical data; India / ethnology; Male; Middle Aged; Self Care / statistics & numerical data; United States   \n",
      "\n",
      "Organize your analysis of this abstract in a table, with the following columns: PMID, title, precision medicine, diabetes, primary study. Please provide evidence directly from the PMID, title, abstract, and MeSH terms to justify your classifications. \n",
      "Now identify the source population. The correct source population is based on the study participants' racial/ethnic background, regardless of geographic location, and refers to humans specifically. Exclude plants and therapeutics that may originate in the region and non-human animal models. The options are humans descending from Central Asia (CA), Central and Eastern Europe (CEE), East Asia (EA), Latin America & Caribbean (LAC), Middle East & North Africa (MENA), South East Asia & Pacific Islands (SEAP), South Asia (SA), Sub-Saharan Africa (SSA), the United States of America and Canada (NA), or Western Europe (WE). \n",
      "\n",
      "Add the classification of the source population to the table in a separate column. You may only use the abbreviation of the source populations provided. \n"
     ]
    }
   ],
   "source": [
    "execfile(path_equity_precision_llm_repo + \"/functions/prompt_generator.py\")\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/base_prompt_append.py\")\n",
    "\n",
    "base_prompt_files = ['p1v6', 'p2v6', 'p3v6','p4v6']\n",
    "base_prompts = base_prompt_append(base_prompt_files)\n",
    "\n",
    "prompt_pmid = prompt_generator(22744164, base_prompts, excel_path)\n",
    "\n",
    "\n",
    "print(base_prompts[0])\n",
    "print(base_prompts[1])\n",
    "print(prompt_pmid)\n",
    "print(base_prompts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing Your Batch File\n",
    "\n",
    "https://platform.openai.com/docs/guides/batch\n",
    "\n",
    ".jsonl file where each line contains the details of an individual request to the API\n",
    "\n",
    "Each request must include a custom_id value\n",
    "\n",
    "{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n",
    "\n",
    "\n",
    "{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an unhelpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_list = pd.read_excel(excel_path, sheet_name='Training Data')['PMID'].tolist()\n",
    "\n",
    "json_list_scenario1 = []\n",
    "json_list_scenario2 = []\n",
    "json_list_scenario3 = []\n",
    "json_list_scenario4 = []\n",
    "\n",
    "for index, pmid in enumerate(pmid_list):\n",
    "    prompt_pmid = prompt_generator(pmid, base_prompts, excel_path)\n",
    "    dict_pmid_scenario1 = {\"custom_id\": str(index) + \"_\" + str(pmid), \n",
    "                 \"method\": \"POST\", \n",
    "                 \"url\": \"/v1/chat/completions\", \n",
    "                 \"body\": {\"model\": \"gpt-4o-mini-2024-07-18\", \n",
    "                          \"messages\": [ {\"role\": \"system\", \"content\": base_prompts[0]},\n",
    "                                        {\"role\": \"system\", \"content\": base_prompts[1]},\n",
    "                                        {\"role\": \"user\", \"content\": prompt_pmid},\n",
    "                                        {\"role\": \"user\", \"content\": base_prompts[3]}],\n",
    "                            \"max_tokens\": 1000,\n",
    "                            \"frequency_penalty\": 0.2,\n",
    "                            \"temperature\": 0.2,\n",
    "                            \"presence_penalty\": 0.3\n",
    "\n",
    "                        }\n",
    "                }\n",
    "    \n",
    "    dict_pmid_scenario2 = {\"custom_id\": str(index) + \"_\" + str(pmid), \n",
    "                 \"method\": \"POST\", \n",
    "                 \"url\": \"/v1/chat/completions\", \n",
    "                 \"body\": {\"model\": \"gpt-4o-mini-2024-07-18\", \n",
    "                          \"messages\": [ {\"role\": \"system\", \"content\": base_prompts[0]},\n",
    "                                        {\"role\": \"system\", \"content\": base_prompts[1]},\n",
    "                                        {\"role\": \"user\", \"content\": prompt_pmid},\n",
    "                                        {\"role\": \"user\", \"content\": base_prompts[3]}],\n",
    "                            \"max_tokens\": 1000,\n",
    "                            \"frequency_penalty\": -0.2,\n",
    "                            \"temperature\": 0.2,\n",
    "                            \"presence_penalty\": -0.3\n",
    "\n",
    "                        }\n",
    "                }\n",
    "    \n",
    "    dict_pmid_scenario3 = {\"custom_id\": str(index) + \"_\" + str(pmid), \n",
    "                 \"method\": \"POST\", \n",
    "                 \"url\": \"/v1/chat/completions\", \n",
    "                 \"body\": {\"model\": \"gpt-4o-mini-2024-07-18\", \n",
    "                          \"messages\": [ {\"role\": \"system\", \"content\": base_prompts[0]},\n",
    "                                        {\"role\": \"system\", \"content\": base_prompts[1]},\n",
    "                                        {\"role\": \"user\", \"content\": prompt_pmid},\n",
    "                                        {\"role\": \"user\", \"content\": base_prompts[3]}],\n",
    "                            \"max_tokens\": 1000,\n",
    "                            \"frequency_penalty\": 0.2,\n",
    "                            \"top_p\": 0.8,\n",
    "                            \"presence_penalty\": 0.3\n",
    "\n",
    "                        }\n",
    "                }\n",
    "\n",
    "    dict_pmid_scenario4 = {\"custom_id\": str(index) + \"_\" + str(pmid), \n",
    "                 \"method\": \"POST\", \n",
    "                 \"url\": \"/v1/chat/completions\", \n",
    "                 \"body\": {\"model\": \"gpt-4o-mini-2024-07-18\", \n",
    "                          \"messages\": [ {\"role\": \"system\", \"content\": base_prompts[0]},\n",
    "                                        {\"role\": \"system\", \"content\": base_prompts[1]},\n",
    "                                        {\"role\": \"user\", \"content\": prompt_pmid},\n",
    "                                        {\"role\": \"user\", \"content\": base_prompts[3]}],\n",
    "                            \"max_tokens\": 1000,\n",
    "                            \"frequency_penalty\": -0.2,\n",
    "                            \"top_p\": 0.8,\n",
    "                            \"presence_penalty\": -0.3\n",
    "\n",
    "                        }\n",
    "                }\n",
    "    json_list_scenario1.append(dict_pmid_scenario1)\n",
    "    \n",
    "    json_list_scenario2.append(dict_pmid_scenario2)\n",
    "\n",
    "    json_list_scenario3.append(dict_pmid_scenario3)\n",
    "\n",
    "    json_list_scenario4.append(dict_pmid_scenario4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 1.jsonl', 'w') as outfile:\n",
    "    for entry in json_list_scenario1:\n",
    "        json_line = json.dumps(entry)\n",
    "        outfile.write(json_line + '\\n')\n",
    "\n",
    "with open(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 2.jsonl', 'w') as outfile:\n",
    "    for entry in json_list_scenario2:\n",
    "        json_line = json.dumps(entry)\n",
    "        outfile.write(json_line + '\\n')\n",
    "\n",
    "with open(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 3.jsonl', 'w') as outfile:\n",
    "    for entry in json_list_scenario3:\n",
    "        json_line = json.dumps(entry)\n",
    "        outfile.write(json_line + '\\n')\n",
    "\n",
    "with open(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 4.jsonl', 'w') as outfile:\n",
    "    for entry in json_list_scenario4:\n",
    "        json_line = json.dumps(entry)\n",
    "        outfile.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Uploading Your Batch Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key= api_key_epl_shared)\n",
    "\n",
    "batch_input_file_scenario1 = client.files.create(\n",
    "  file=open(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 1.jsonl', \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_input_file_scenario2 = client.files.create(\n",
    "  file=open(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 2.jsonl', \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_input_file_scenario3 = client.files.create(\n",
    "  file=open(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 3.jsonl', \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_input_file_scenario4 = client.files.create(\n",
    "  file=open(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 4.jsonl', \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-1JSSKE7DyJtoH6NgyfaKpT\n",
      "file-1JSSKE7DyJtoH6NgyfaKpT\n",
      "file-AixGuD6tJngzTARxKwMrDS\n",
      "file-3NUegcGHJZzggrTmrDZgM3\n"
     ]
    }
   ],
   "source": [
    "batch_input_file_scenario1_id = batch_input_file_scenario1.id\n",
    "print(batch_input_file_scenario1_id)\n",
    "batch_created_scenario1 = client.batches.create(\n",
    "    input_file_id=batch_input_file_scenario1_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"training data for PMID query - scenario 1\"\n",
    "    }\n",
    ")\n",
    "\n",
    "batch_input_file_scenario2_id = batch_input_file_scenario2.id\n",
    "print(batch_input_file_scenario1_id)\n",
    "batch_created_scenario2 = client.batches.create(\n",
    "    input_file_id=batch_input_file_scenario2_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"training data for PMID query - scenario 2\"\n",
    "    }\n",
    ")\n",
    "\n",
    "batch_input_file_scenario3_id = batch_input_file_scenario3.id\n",
    "print(batch_input_file_scenario3_id)\n",
    "batch_created_scenario3 = client.batches.create(\n",
    "    input_file_id=batch_input_file_scenario3_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"training data for PMID query - scenario 3\"\n",
    "    }\n",
    ")\n",
    "\n",
    "batch_input_file_scenario4_id = batch_input_file_scenario4.id\n",
    "print(batch_input_file_scenario4_id)\n",
    "batch_created_scenario4 = client.batches.create(\n",
    "    input_file_id=batch_input_file_scenario4_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"training data for PMID query - scenario 4\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_file_path = path_equity_precision_llm_repo + \"/Batches.txt\"\n",
    "\n",
    "if not os.path.exists(batches_file_path):\n",
    "    with open(batches_file_path, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "with open(batches_file_path, \"a\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    current_time = datetime.datetime.now()\n",
    "    f.write(f\"# {current_time}\\n\")\n",
    "    f.write(f\"batch_created_id_scenario1 = '{batch_created_scenario1.id}'\\n\")\n",
    "    f.write(f\"batch_created_id_scenario2 = '{batch_created_scenario2.id}'\\n\")\n",
    "    f.write(f\"batch_created_id_scenario3 = '{batch_created_scenario3.id}'\\n\")\n",
    "    f.write(f\"batch_created_id_scenario4 = '{batch_created_scenario4.id}'\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_created_scenario1_id = batch_created_scenario1.id\n",
    "batch_created_scenario2_id = batch_created_scenario2.id\n",
    "batch_created_scenario3_id = batch_created_scenario3.id\n",
    "batch_created_scenario4_id = batch_created_scenario4.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Checking the Status of a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1: completed; Scenario 2: completed; Scenario 3:completed; Scenario 4:in_progress\n"
     ]
    }
   ],
   "source": [
    "# batch_created_id = 'batch_675d9dc3f25881909544964060c659c3'\n",
    "client = OpenAI(api_key= api_key_epl_shared)\n",
    "\n",
    "batch_status_scenario1 = client.batches.retrieve(batch_created_scenario1_id)\n",
    "\n",
    "batch_status_scenario2 = client.batches.retrieve(batch_created_scenario2_id)\n",
    "batch_status_scenario3 = client.batches.retrieve(batch_created_scenario3_id)\n",
    "\n",
    "batch_status_scenario4 = client.batches.retrieve(batch_created_scenario4_id)\n",
    "\n",
    "print(\"Scenario 1: \" + batch_status_scenario1.status + \"; Scenario 2: \" + batch_status_scenario2.status + \"; Scenario 3:\" + batch_status_scenario3.status + \"; Scenario 4:\" + batch_status_scenario4.status)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retrieving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a non-empty value for `file_id` but received None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\external\\equity_precision_llm\\preprocessing\\epl02_openai api query.ipynb Cell 18\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/external/equity_precision_llm/preprocessing/epl02_openai%20api%20query.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m file_response_scenario2 \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mfiles\u001b[39m.\u001b[39mcontent(batch_status_scenario2\u001b[39m.\u001b[39moutput_file_id)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/external/equity_precision_llm/preprocessing/epl02_openai%20api%20query.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m file_response_scenario3 \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mfiles\u001b[39m.\u001b[39mcontent(batch_status_scenario3\u001b[39m.\u001b[39moutput_file_id)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/code/external/equity_precision_llm/preprocessing/epl02_openai%20api%20query.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m file_response_scenario4 \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mfiles\u001b[39m.\u001b[39;49mcontent(batch_status_scenario4\u001b[39m.\u001b[39;49moutput_file_id)\n",
      "File \u001b[1;32mc:\\Users\\jvargh7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\resources\\files.py:288\u001b[0m, in \u001b[0;36mFiles.content\u001b[1;34m(self, file_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39mReturns the contents of the specified file.\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file_id:\n\u001b[1;32m--> 288\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a non-empty value for `file_id` but received \u001b[39m\u001b[39m{\u001b[39;00mfile_id\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m extra_headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mAccept\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mapplication/binary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(extra_headers \u001b[39mor\u001b[39;00m {})}\n\u001b[0;32m    290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get(\n\u001b[0;32m    291\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/files/\u001b[39m\u001b[39m{\u001b[39;00mfile_id\u001b[39m}\u001b[39;00m\u001b[39m/content\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    292\u001b[0m     options\u001b[39m=\u001b[39mmake_request_options(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m     cast_to\u001b[39m=\u001b[39m_legacy_response\u001b[39m.\u001b[39mHttpxBinaryResponseContent,\n\u001b[0;32m    296\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected a non-empty value for `file_id` but received None"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key= api_key_epl_shared)\n",
    "\n",
    "file_response_scenario1 = client.files.content(batch_status_scenario1.output_file_id)\n",
    "file_response_scenario2 = client.files.content(batch_status_scenario2.output_file_id)\n",
    "file_response_scenario3 = client.files.content(batch_status_scenario3.output_file_id)\n",
    "file_response_scenario4 = client.files.content(batch_status_scenario4.output_file_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:/code/external/equity_precision_llm/functions/format_gpt_output.py:5: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_json(json_output.content.decode('utf-8'), lines=True)\n",
      "C:/code/external/equity_precision_llm/functions/format_gpt_output.py:5: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_json(json_output.content.decode('utf-8'), lines=True)\n",
      "C:/code/external/equity_precision_llm/functions/format_gpt_output.py:5: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_json(json_output.content.decode('utf-8'), lines=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'file_response_scenario4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\external\\equity_precision_llm\\preprocessing\\epl02_openai api query.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/external/equity_precision_llm/preprocessing/epl02_openai%20api%20query.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m results2 \u001b[39m=\u001b[39m format_gpt_output(file_response_scenario2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/external/equity_precision_llm/preprocessing/epl02_openai%20api%20query.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m results3 \u001b[39m=\u001b[39m format_gpt_output(file_response_scenario3)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/code/external/equity_precision_llm/preprocessing/epl02_openai%20api%20query.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m results4 \u001b[39m=\u001b[39m format_gpt_output(file_response_scenario4)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/external/equity_precision_llm/preprocessing/epl02_openai%20api%20query.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m results1\u001b[39m.\u001b[39mto_csv(path_equity_precision_llm_folder \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mllm training\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTraining Scenario 1_results.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/external/equity_precision_llm/preprocessing/epl02_openai%20api%20query.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m results2\u001b[39m.\u001b[39mto_csv(path_equity_precision_llm_folder \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mllm training\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTraining Scenario 2_results.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_response_scenario4' is not defined"
     ]
    }
   ],
   "source": [
    "execfile(path_equity_precision_llm_repo + \"/functions/format_gpt_output.py\")\n",
    "\n",
    "results1 = format_gpt_output(file_response_scenario1)\n",
    "results2 = format_gpt_output(file_response_scenario2)\n",
    "results3 = format_gpt_output(file_response_scenario3)\n",
    "results4 = format_gpt_output(file_response_scenario4)\n",
    "\n",
    "\n",
    "\n",
    "results1.to_csv(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 1_results.csv', index=False)\n",
    "results2.to_csv(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 2_results.csv', index=False)\n",
    "results3.to_csv(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 3_results.csv', index=False)\n",
    "results4.to_csv(path_equity_precision_llm_folder + '\\llm training\\Training Scenario 4_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\external\\equity_precision_llm\\preprocessing\\epl02_openai api query.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/code/external/equity_precision_llm/preprocessing/epl02_openai%20api%20query.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results4\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results4' is not defined"
     ]
    }
   ],
   "source": [
    "results4.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
