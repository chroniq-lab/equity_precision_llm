{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn as sklearn\n",
    "import os as os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re as re\n",
    "\n",
    "# if os.getlogin()==\"JVARGH7\":\n",
    "#    path_equity_precision_llm_folder = \"C:/Cloud/OneDrive - Emory University/Papers/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "#    path_equity_precision_llm_repo =  'C:/code/external/equity_precision_llm'\n",
    "\n",
    "# elif os.getlogin()=='aamnasoniwala':\n",
    "#    path_equity_precision_llm_folder = \"/Users/aamnasoniwala/Library/CloudStorage/OneDrive-Emory/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "#    path_equity_precision_llm_repo = '/Users/aamnasoniwala/code/equity_precision_llm'\n",
    "\n",
    "path_equity_precision_llm_folder = \"/Users/aamnasoniwala/Library/CloudStorage/OneDrive-Emory/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "path_equity_precision_llm_repo = '/Users/aamnasoniwala/code/equity_precision_llm'\n",
    "\n",
    "excel_path = path_equity_precision_llm_folder + \"/llm training/Test Data Splits/Test Part \"\n",
    "# path_equity_precision_llm_repo = os.path.abspath(\"\").replace(\"preprocessing\", \"\")\n",
    "\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/crosstab_summary.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path_test = path_equity_precision_llm_folder + \"/llm training/epldat03_Test Data.xlsx\"\n",
    "# path_equity_precision_llm_repo = os.path.abspath(\"\").replace(\"preprocessing\", \"\")\n",
    "\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/clean_input.py\")\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/crosstab_summary.py\")\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/standardize_population.py\")\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/adjusted_source_population_match.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>MeSH</th>\n",
       "      <th>Source Population</th>\n",
       "      <th>Precision Medicine</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Correct Source Population</th>\n",
       "      <th>Primary Study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23680249</td>\n",
       "      <td>Pattern and predictors of dyslipidemia in pati...</td>\n",
       "      <td>Dyslipidemia is a major risk factor for macro-...</td>\n",
       "      <td>Adult, Blood Glucose, Cross-Sectional Studies,...</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SA</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32585310</td>\n",
       "      <td>Proteome-wide assessment of diabetes mellitus ...</td>\n",
       "      <td>Proteomics is expected to provide novel insigh...</td>\n",
       "      <td>Adult, Blood Proteins, Case-Control Studies, C...</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SA</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34254537</td>\n",
       "      <td>Role of Nutrition Counseling and Lifestyle Mod...</td>\n",
       "      <td>Objective: India is the second country after C...</td>\n",
       "      <td>Blood Glucose, Counseling, Diabetes Mellitus, ...</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SA</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15287926</td>\n",
       "      <td>Lipids and lipoprotein(a) concentrations in Pa...</td>\n",
       "      <td>The aim of the present study was to analyze se...</td>\n",
       "      <td>Blood Pressure, Case-Control Studies, Diabetes...</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SA</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16108841</td>\n",
       "      <td>Telomere shortening occurs in Asian Indian Typ...</td>\n",
       "      <td>Telomere shortening has been reported in sever...</td>\n",
       "      <td>Aging, Cholesterol, Diabetes Mellitus, Type 2,...</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SA</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PMID                                              Title  \\\n",
       "0  23680249  Pattern and predictors of dyslipidemia in pati...   \n",
       "1  32585310  Proteome-wide assessment of diabetes mellitus ...   \n",
       "2  34254537  Role of Nutrition Counseling and Lifestyle Mod...   \n",
       "3  15287926  Lipids and lipoprotein(a) concentrations in Pa...   \n",
       "4  16108841  Telomere shortening occurs in Asian Indian Typ...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Dyslipidemia is a major risk factor for macro-...   \n",
       "1  Proteomics is expected to provide novel insigh...   \n",
       "2  Objective: India is the second country after C...   \n",
       "3  The aim of the present study was to analyze se...   \n",
       "4  Telomere shortening has been reported in sever...   \n",
       "\n",
       "                                                MeSH Source Population  \\\n",
       "0  Adult, Blood Glucose, Cross-Sectional Studies,...        South Asia   \n",
       "1  Adult, Blood Proteins, Case-Control Studies, C...        South Asia   \n",
       "2  Blood Glucose, Counseling, Diabetes Mellitus, ...        South Asia   \n",
       "3  Blood Pressure, Case-Control Studies, Diabetes...        South Asia   \n",
       "4  Aging, Cholesterol, Diabetes Mellitus, Type 2,...        South Asia   \n",
       "\n",
       "  Precision Medicine Diabetes Correct Source Population Primary Study  \n",
       "0                 No      Yes                        SA           Yes  \n",
       "1                 No      Yes                        SA           Yes  \n",
       "2                 No      Yes                        SA           Yes  \n",
       "3                 No      Yes                        SA           Yes  \n",
       "4                 No      Yes                        SA           Yes  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = pd.read_excel(excel_path_test)\n",
    "input_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_json_splits = 4\n",
    "# Concatenate multiple CSVs\n",
    "results = None\n",
    "for scenario in range(1, n_json_splits + 1):\n",
    "    file_path = f\"{path_equity_precision_llm_folder}/llm training/Test Data Splits/Test Part {scenario}_results.csv\"\n",
    "    temp_df = pd.read_csv(file_path)\n",
    "    temp_df = temp_df[['pmid', 'title', 'gpt_precision_medicine', 'gpt_diabetes', 'gpt_primary_study', 'gpt_source_population']]\n",
    "\n",
    "    if results is None:\n",
    "        results = temp_df\n",
    "    else:\n",
    "        results = pd.concat([results, temp_df], ignore_index=True)\n",
    "\n",
    "# Merge datasets\n",
    "merged_df_test = input_test.merge(results, left_on='PMID', right_on='pmid', how='left')\n",
    "merged_df_test['gpt_source_population'] = merged_df_test['gpt_source_population'].apply(standardize_population)\n",
    "\n",
    "merged_df_test.to_csv(f\"{path_equity_precision_llm_folder}/llm training/Test Scenario 4_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2091\n",
      "Number of columns: 15\n"
     ]
    }
   ],
   "source": [
    "rows, columns = merged_df_test.shape\n",
    "print(f\"Number of rows: {rows}\")\n",
    "print(f\"Number of columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in new dataframe: 2211\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file_names = [\n",
    "    f\"{path_equity_precision_llm_folder}/llm training/Training Scenario 4_results.csv\",\n",
    "    f\"{path_equity_precision_llm_folder}/llm training/Development Scenario 4_results.csv\",\n",
    "    f\"{path_equity_precision_llm_folder}/llm training/Test Scenario 4_results.csv\"\n",
    "]\n",
    "\n",
    "# Columns to keep\n",
    "kept_columns = [\n",
    "    'pmid', 'title', 'gpt_precision_medicine', \n",
    "    'gpt_diabetes', 'gpt_primary_study', 'gpt_source_population'\n",
    "]\n",
    "\n",
    "# Read and concatenate all data into a new df\n",
    "input_all_scenario4 = pd.concat(\n",
    "    [pd.read_csv(file)[kept_columns] for file in file_names],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Check\n",
    "print(f\"Total rows in new dataframe: {input_all_scenario4.shape[0]}\")\n",
    "\n",
    "# Save df\n",
    "output_path = f\"{path_equity_precision_llm_folder}/llm training/input_all_scenario4.csv\"\n",
    "input_all_scenario4.to_csv(output_path, index=False)\n",
    "\n",
    "# Apply standardize_population function\n",
    "input_all_scenario4['gpt_source_population'] = input_all_scenario4['gpt_source_population'].apply(standardize_population)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of studies excluded due to missing Abstract: 1\n",
      "Remaining studies after exclusion: 2210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2210, 6)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the valid source population categories\n",
    "VALID_REGIONS = {\"ca\", \"na\", \"cee\", \"lac\", \"sa\", \"ssa\", \"we\", \"seap\", \"mena\", \"ea\", \"Unknown\"}\n",
    "EUROPEAN_REGIONS = {\"we\", \"na\"}\n",
    "NON_EUROPEAN_REGIONS = VALID_REGIONS - EUROPEAN_REGIONS - {\"Unknown\"}\n",
    "\n",
    "# Step 1: Identify and count excluded studies due to missing Title or Abstract\n",
    "def is_missing(value):\n",
    "    return pd.isna(value) or str(value).strip() == \"\"\n",
    "\n",
    "def excluded(row):\n",
    "    # return is_missing(row['Title'])\n",
    "    return is_missing(row['title']) \n",
    "\n",
    "# Count excluded studies\n",
    "excluded_studies_df = input_all_scenario4[input_all_scenario4.apply(excluded, axis=1)]\n",
    "excluded_count = len(excluded_studies_df)\n",
    "print(f\"Number of studies excluded due to missing Abstract: {excluded_count}\")\n",
    "\n",
    "# Apply exclusion\n",
    "input_all_scenario4_after_exclusion = input_all_scenario4.drop(excluded_studies_df.index)\n",
    "\n",
    "# Verify remaining studies\n",
    "remaining_studies = len(input_all_scenario4_after_exclusion)\n",
    "print(f\"Remaining studies after exclusion: {remaining_studies}\")\n",
    "\n",
    "input_all_scenario4_after_exclusion.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where normalized_regions has more than 1 element: 123\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Normalize source population categories\n",
    "\n",
    "def normalize_source_population(source_population):\n",
    "    if pd.isna(source_population):\n",
    "        return set()\n",
    "    return VALID_REGIONS.intersection(set(re.sub(r'\\s+', '', source_population).split(',')))\n",
    "\n",
    "input_all_scenario4_after_exclusion['normalized_regions'] = input_all_scenario4_after_exclusion['gpt_source_population'].apply(normalize_source_population)\n",
    "\n",
    "# Count rows where normalized_regions has more than 1 element\n",
    "multi_region_rows = input_all_scenario4_after_exclusion[input_all_scenario4_after_exclusion['normalized_regions'].apply(lambda x: len(x) > 1)]\n",
    "multi_region_count = len(multi_region_rows)\n",
    "print(f\"Number of rows where normalized_regions has more than 1 element: {multi_region_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2447, 7)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Expand data **only** for regional counts\n",
    "expanded_rows = []\n",
    "for _, row in input_all_scenario4_after_exclusion.iterrows():\n",
    "    for region in row['normalized_regions']:\n",
    "        new_row = row.copy()\n",
    "        new_row['gpt_source_population'] = region\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Create expanded DataFrame for regional counts\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "expanded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total Precision Medicine': np.int64(2210),\n",
       " 'Total Diabetes': np.int64(2210),\n",
       " 'Total Primary Study': np.int64(2210)}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Create a crosstab summary function\n",
    "def crosstab_summary(expanded_df, categories, region_col='gpt_source_population'):\n",
    "    expanded_df['combined_category'] = expanded_df[categories].astype(str).agg('_'.join, axis=1)\n",
    "    summary = pd.crosstab(expanded_df['combined_category'], expanded_df[region_col])\n",
    "    summary.reset_index(inplace=True)\n",
    "    return summary\n",
    "\n",
    "# Define categories\n",
    "categories = ['gpt_precision_medicine', 'gpt_diabetes', 'gpt_primary_study']\n",
    "\n",
    "# Generate crosstab for regional counts\n",
    "summary_expanded_df = crosstab_summary(expanded_df, categories, region_col='gpt_source_population')\n",
    "\n",
    "# Step 5: Compute overall totals without double-counting studies\n",
    "total_counts = {\n",
    "    'Total Precision Medicine': input_all_scenario4_after_exclusion['gpt_precision_medicine'].notna().sum(),\n",
    "    'Total Diabetes': input_all_scenario4_after_exclusion['gpt_diabetes'].notna().sum(),\n",
    "    'Total Primary Study': input_all_scenario4_after_exclusion['gpt_primary_study'].notna().sum()\n",
    "}\n",
    "\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>gpt_source_population</th>\n",
       "      <th>combined_category</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>ca</th>\n",
       "      <th>cee</th>\n",
       "      <th>ea</th>\n",
       "      <th>lac</th>\n",
       "      <th>mena</th>\n",
       "      <th>na</th>\n",
       "      <th>sa</th>\n",
       "      <th>seap</th>\n",
       "      <th>ssa</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_no_no</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_no_yes</td>\n",
       "      <td>215</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_yes_no</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_yes_not determined</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no_yes_yes</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>88</td>\n",
       "      <td>52</td>\n",
       "      <td>36</td>\n",
       "      <td>56</td>\n",
       "      <td>50</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes_no_no</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes_no_unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yes_no_yes</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yes_yes_no</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yes_yes_not determinable</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yes_yes_yes</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>206</td>\n",
       "      <td>190</td>\n",
       "      <td>19</td>\n",
       "      <td>137</td>\n",
       "      <td>165</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "gpt_source_population         combined_category  Unknown  ca  cee   ea  lac  \\\n",
       "0                                      no_no_no       81   1    1    9    7   \n",
       "1                                     no_no_yes      215   6   51   70   67   \n",
       "2                                     no_yes_no        1   2    5   13    1   \n",
       "3                         no_yes_not determined        1   0    0    0    0   \n",
       "4                                    no_yes_yes       48   3   30   88   52   \n",
       "5                                     yes_no_no        3   0    0    1    3   \n",
       "6                                yes_no_unknown        0   0    0    0    1   \n",
       "7                                    yes_no_yes       16   0   13   69   67   \n",
       "8                                    yes_yes_no        0   0    1    9   10   \n",
       "9                      yes_yes_not determinable        0   0    0    0    0   \n",
       "10                                  yes_yes_yes       19   2   46  206  190   \n",
       "\n",
       "gpt_source_population  mena   na   sa  seap  ssa  we  \n",
       "0                         2   19    5     1    4  10  \n",
       "1                        18   50   26    20    9  25  \n",
       "2                         1   11   15     3    1   9  \n",
       "3                         0    0    0     0    0   0  \n",
       "4                        36   56   50    35   16  30  \n",
       "5                         0    5    4     0    0   4  \n",
       "6                         0    0    0     0    0   0  \n",
       "7                         7   34   33    13    4  12  \n",
       "8                         0   12   13     3    2   4  \n",
       "9                         0    0    1     0    0   0  \n",
       "10                       19  137  165    39   24  52  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Save the expanded output for regions\n",
    "output_path = path_equity_precision_llm_repo + '/preprocessing/2202 manually screened.csv'\n",
    "summary_expanded_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display summary\n",
    "display(summary_expanded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Categorize studies into European vs. Non-European vs. Unknown (without duplicates)\n",
    "def classify_region(regions):\n",
    "    classification = set()\n",
    "    if any(r in EUROPEAN_REGIONS for r in regions):\n",
    "        classification.add('European')\n",
    "    if any(r in NON_EUROPEAN_REGIONS for r in regions):\n",
    "        classification.add('Non-European')\n",
    "    if not classification:\n",
    "        classification.add('Unknown')\n",
    "    return classification\n",
    "\n",
    "# Remove rows where gpt_source_population is 'Unknown'\n",
    "\n",
    "\n",
    "input_all_scenario4_after_exclusion['classification_group'] = input_all_scenario4_after_exclusion['normalized_regions'].apply(classify_region)\n",
    "\n",
    "input_all_scenario4_after_exclusion_region = input_all_scenario4_after_exclusion[input_all_scenario4_after_exclusion['gpt_source_population'] != 'Unknown']\n",
    "\n",
    "# Compute total counts per classification group\n",
    "group_totals = input_all_scenario4_after_exclusion_region['classification_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expanded_rows_region = []\n",
    "for _, row in input_all_scenario4_after_exclusion_region.iterrows():\n",
    "    for group in row['classification_group']:\n",
    "        new_row = row.copy()\n",
    "        new_row['classification_group'] = group\n",
    "        expanded_rows_region.append(new_row)\n",
    "\n",
    "# Create expanded DataFrame for regional counts\n",
    "expanded_df_region = pd.DataFrame(expanded_rows_region)\n",
    "expanded_df_region.shape\n",
    "\n",
    "\n",
    "summary_expanded_df_region = crosstab_summary(expanded_df_region, categories, region_col='classification_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>classification_group</th>\n",
       "      <th>combined_category</th>\n",
       "      <th>European</th>\n",
       "      <th>Non-European</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_no_no</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_no_yes</td>\n",
       "      <td>74</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_yes_no</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_yes_yes</td>\n",
       "      <td>83</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes_no_no</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes_no_unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes_no_yes</td>\n",
       "      <td>42</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yes_yes_no</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yes_yes_not determinable</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yes_yes_yes</td>\n",
       "      <td>176</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "classification_group         combined_category  European  Non-European\n",
       "0                                     no_no_no        24            25\n",
       "1                                    no_no_yes        74           257\n",
       "2                                    no_yes_no        18            33\n",
       "3                                   no_yes_yes        83           297\n",
       "4                                    yes_no_no         7             8\n",
       "5                               yes_no_unknown         0             1\n",
       "6                                   yes_no_yes        42           194\n",
       "7                                   yes_yes_no        12            27\n",
       "8                     yes_yes_not determinable         0             1\n",
       "9                                  yes_yes_yes       176           637"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 8: Save the expanded output for regions\n",
    "output_path_group = path_equity_precision_llm_repo + '/preprocessing/2202 manually screened.csv'\n",
    "summary_expanded_df_region.to_csv(output_path_group, index=False)\n",
    "\n",
    "# Display summary\n",
    "display(summary_expanded_df_region)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
