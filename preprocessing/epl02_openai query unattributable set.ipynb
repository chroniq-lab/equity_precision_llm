{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn as sklearn\n",
    "import os as os\n",
    "import getpass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re as re\n",
    "import datetime as datetime\n",
    "# Ensure all necessary packages are installed\n",
    "%pip install kneed\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "if user == \"JVARGH7\":\n",
    "    path_equity_precision_llm_folder = \"C:/Cloud/OneDrive - Emory University/Papers/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "    path_equity_precision_llm_repo =  'C:/code/external/equity_precision_llm'\n",
    "\n",
    "elif user == \"aamnasoniwala\":\n",
    "    path_equity_precision_llm_folder = \"/Users/aamnasoniwala/Library/CloudStorage/OneDrive-Emory/Global Equity in Diabetes Precision Medicine LLM\"\n",
    "    path_equity_precision_llm_repo = '/Users/aamnasoniwala/code/equity_precision_llm'\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unrecognized user\")\n",
    "\n",
    "excel_path = path_equity_precision_llm_folder + \"/llm training/Test Data.xlsx\"\n",
    "# path_equity_precision_llm_repo = os.path.abspath(\"\").replace(\"preprocessing\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_epl_shared = \"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "# https://stackoverflow.com/questions/36959031/how-to-source-file-into-python-script\n",
    "execfile(path_equity_precision_llm_repo + \"/constants.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execfile(path_equity_precision_llm_repo + \"/functions/prompt_generator.py\")\n",
    "execfile(path_equity_precision_llm_repo + \"/functions/base_prompt_append.py\")\n",
    "\n",
    "base_prompt_files = ['p1v6', 'p2v6', 'p3v6','p4v6']\n",
    "base_prompts = base_prompt_append(base_prompt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_list = pd.read_excel(excel_path, sheet_name='Sheet1')['PMID'].tolist()\n",
    "\n",
    "json_list_test = []\n",
    "\n",
    "for index, pmid in enumerate(pmid_list):\n",
    "    prompt_pmid = prompt_generator(pmid, base_prompts, excel_path, sheet_name='Sheet1')\n",
    "    dict_pmid_test = {\"custom_id\": str(index) + \"_\" + str(pmid), \n",
    "                 \"method\": \"POST\", \n",
    "                 \"url\": \"/v1/chat/completions\", \n",
    "                 \"body\": {\"model\": \"gpt-4o-2024-08-06\", \n",
    "                          \"messages\": [ {\"role\": \"system\", \"content\": base_prompts[0]},\n",
    "                                        {\"role\": \"system\", \"content\": base_prompts[1]},\n",
    "                                        {\"role\": \"user\", \"content\": prompt_pmid},\n",
    "                                        {\"role\": \"user\", \"content\": base_prompts[3]}],\n",
    "                            \"max_tokens\": 2000,\n",
    "                            \"frequency_penalty\": -0.2,\n",
    "                            \"top_p\": 0.8,\n",
    "                            \"presence_penalty\": -0.3\n",
    "\n",
    "                        }\n",
    "                }\n",
    "\n",
    "    json_list_test.append(dict_pmid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove everything inside the folder 'Test Data Splits'\n",
    "\n",
    "\n",
    "for filename in os.listdir(path_equity_precision_llm_folder + \"/llm training/Test Data Splits\"):\n",
    "    os.remove(path_equity_precision_llm_folder + \"/llm training/Test Data Splits\" +\"/\" + filename)\n",
    "\n",
    "# Number of splits\n",
    "n_json_splits = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Split the JSON data\n",
    "split_index = len(json_list_test) // n_json_splits # n_json_splits to be specified above\n",
    "\n",
    "for i in range(n_json_splits):\n",
    "    part = json_list_test[i*split_index:(i+1)*split_index]\n",
    "    with(open(path_equity_precision_llm_folder + '/llm training/Test Data Splits/' + f\"Test_part{i+1}.jsonl\", 'w')) as f:\n",
    "        for entry in part:\n",
    "            json_line = json.dumps(entry)\n",
    "            f.write(json_line + '\\n')\n",
    "    print(f\"JSON file saved successfully to OneDrive folder:\" + f\"Test_part{i+1}.jsonl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key= api_key_epl_shared)\n",
    "\n",
    "inputs_file_path = path_equity_precision_llm_repo + \"/Inputs.txt\"\n",
    "\n",
    "if not os.path.exists(inputs_file_path):\n",
    "    with open(inputs_file_path, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "\n",
    "with open(inputs_file_path, \"a\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    current_time = datetime.datetime.now()\n",
    "    f.write(f\"# {current_time}\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "for i in range(n_json_splits):\n",
    "    batch_input_file_test_partI = client.files.create(\n",
    "      file = open(path_equity_precision_llm_folder + '/llm training/Test Data Splits/' + f\"Test_part{i+1}.jsonl\",\"rb\"),\n",
    "      purpose=\"batch\"\n",
    "    )\n",
    "    \n",
    "    with open(inputs_file_path, \"a\") as f:\n",
    "      f.write(f\"batch_input_file_test_part{i+1} = '{batch_input_file_test_partI.id}'\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the batch_input_file_test_partI_id from the Inputs.txt file\n",
    "batch_input_file_test_part4_id = \"file-MEH4Ly2TgTfoVRs283XvZJ\"\n",
    "part = 4 # This needs to be changed to the part number\n",
    "\n",
    "\n",
    "print(batch_input_file_test_part4_id)\n",
    "batch_created_test_part4 = client.batches.create(\n",
    "    input_file_id=batch_input_file_test_part4_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"test data for PMID query - scenario 4 part 4\" + str(part)\n",
    "    }\n",
    ")\n",
    "\n",
    "batches_file_path = path_equity_precision_llm_repo + \"/Batches.txt\"\n",
    "\n",
    "if not os.path.exists(batches_file_path):\n",
    "    with open(batches_file_path, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "with open(batches_file_path, \"a\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    current_time = datetime.datetime.now()\n",
    "    f.write(f\"# {current_time}\\n\")\n",
    "    f.write(f\"batch_created_test_part{part}_id = '{batch_created_test_part4.id}'\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_created_test_partI_id = batch_created_test_partI.id\n",
    "batch_created_test_part2_id = 'batch_67b679736cb88190826795d4d28b08fd'\n",
    "part = 2 # This needs to be changed to the part number\n",
    "client = OpenAI(api_key= api_key_epl_shared)\n",
    "\n",
    "\n",
    "batch_status_test_part2 = client.batches.retrieve(batch_created_test_part2_id)\n",
    "\n",
    "print(\"Scenario 4: \" + batch_status_test_part2.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_created_test_part1_id = 'batch_67b66cf9833c8190a3e67e081f9298c9'\n",
    "batch_created_test_part2_id = 'batch_67b679736cb88190826795d4d28b08fd'\n",
    "batch_created_test_part3_id = 'batch_67b6802fec0481908c84063ba49399fc'\n",
    "batch_created_test_part4_id = 'batch_67b682351dc081909e053ce05d48278e'\n",
    "\n",
    "client = OpenAI(api_key= api_key_epl_shared)\n",
    "\n",
    "file_response_test_part1 = client.files.content(batch_status_test_part1.output_file_id)\n",
    "file_response_test_part2 = client.files.content(batch_status_test_part2.output_file_id)\n",
    "file_response_test_part3 = client.files.content(batch_status_test_part3.output_file_id)\n",
    "file_response_test_part4 = client.files.content(batch_status_test_part4.output_file_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execfile(path_equity_precision_llm_repo + \"/functions/format_gpt_output.py\")\n",
    "\n",
    "results4_part1 = format_gpt_output(file_response_test_part1)\n",
    "results4_part1.to_csv(path_equity_precision_llm_folder + '/llm training/Test Data Splits/Test Part ' + str(1) +'_results.csv', index=False)\n",
    "\n",
    "results4_part2 = format_gpt_output(file_response_test_part2)\n",
    "results4_part2.to_csv(path_equity_precision_llm_folder + '/llm training/Test Data Splits/Test Part ' + str(2) +'_results.csv', index=False)\n",
    "\n",
    "results4_part3 = format_gpt_output(file_response_test_part3)\n",
    "results4_part3.to_csv(path_equity_precision_llm_folder + '/llm training/Test Data Splits/Test Part ' + str(3) +'_results.csv', index=False)\n",
    "\n",
    "results4_part4 = format_gpt_output(file_response_test_part4)\n",
    "results4_part4.to_csv(path_equity_precision_llm_folder + '/llm training/Test Data Splits/Test Part ' + str(4) +'_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
